{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os.path\n",
        "import sys\n",
        "import random\n",
        "from operator import itemgetter\n",
        "from collections import defaultdict\n",
        "#----------------------------------------\n",
        "#  Data input\n",
        "#----------------------------------------\n",
        "\n",
        "# Read a text file into a corpus (list of sentences (which in turn are lists of words))\n",
        "# (taken from nested section of HW0)\n",
        "\n",
        "def readFileToCorpus(file_path):\n",
        "    \"\"\" Reads in the text file file_path which contains one sentence per line.\n",
        "    \"\"\"\n",
        "    if os.path.isfile(file_path):\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            i = 0  # This is just a counter to keep track of the sentence numbers\n",
        "            corpus = []  # This will become a list of sentences\n",
        "            print(\"Reading file \", file_path)\n",
        "            for line in file:\n",
        "                i += 1\n",
        "                sentence = line.split()  # Split the line into a list of words\n",
        "                # Append this list as an element to the list of sentences\n",
        "                corpus.append(sentence)\n",
        "                if i % 1000 == 0:\n",
        "                    # Print a status message: str(i) turns int i into a string\n",
        "                    # so we can concatenate it\n",
        "                    sys.stderr.write(\"Reading sentence \" + str(i) + \"\\n\")\n",
        "            # end for\n",
        "        return corpus\n",
        "    else:\n",
        "        # Ideally, we would throw an exception here, but this will suffice\n",
        "        print(\"Error: corpus file \", file_path, \" does not exist\")\n",
        "        sys.exit()  # Exit the script\n",
        "    # end if\n",
        "# end def\n",
        "\n",
        "\n",
        "\n",
        "# Preprocess the corpus\n",
        "from collections import defaultdict\n",
        "\n",
        "def preprocess(corpus):\n",
        "    UNK = '<UNK>'  # Assuming UNK is a constant string\n",
        "    start = '<start>'  # Assuming start is a constant string\n",
        "    end = '<end>'  # Assuming end is a constant string\n",
        "\n",
        "    # Find all the rare words\n",
        "    freqDict = defaultdict(int)\n",
        "    for sen in corpus:\n",
        "        for word in sen:\n",
        "            freqDict[word] += 1\n",
        "    # endfor\n",
        "\n",
        "    # Replace rare words with UNK\n",
        "    for sen in corpus:\n",
        "        for i in range(0, len(sen)):\n",
        "            word = sen[i]\n",
        "            print(word)\n",
        "            print(freqDict[word])\n",
        "            if freqDict[word] < 2:\n",
        "                sen[i] = UNK\n",
        "            # endif\n",
        "        # endfor\n",
        "    # endfor\n",
        "\n",
        "    # Bookend the sentences with start and end tokens\n",
        "    for sen in corpus:\n",
        "        sen.insert(0, start)\n",
        "        sen.append(end)\n",
        "    # endfor\n",
        "\n",
        "    return corpus\n",
        "# enddef\n",
        "\n",
        "def preprocessTest(vocab, corpus):\n",
        "    #replace test words that were unseen in the training with unk\n",
        "    for sen in corpus:\n",
        "        for i in range(0, len(sen)):\n",
        "            word = sen[i]\n",
        "            if word not in vocab:\n",
        "                sen[i] = UNK\n",
        "\t    #endif\n",
        "\t#endfor\n",
        "    #endfor\n",
        "\n",
        "    #bookend the sentences with start and end tokens\n",
        "    for sen in corpus:\n",
        "        sen.insert(0, start)\n",
        "        sen.append(end)\n",
        "    #endfor\n",
        "\n",
        "    return corpus\n",
        "#enddef\n",
        "\n",
        "# Constants\n",
        "UNK = \"UNK\"     # Unknown word token\n",
        "start = \"<s>\"   # Start-of-sentence token\n",
        "end = \"</s>\"    # End-of-sentence-token\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------\n",
        "# Language models and data structures\n",
        "#--------------------------------------------------------------\n",
        "\n",
        "# Parent class for the three language models you need to implement\n",
        "class LanguageModel:\n",
        "    # Initialize and train the model (ie, estimate the model's underlying probability\n",
        "    # distribution from the training corpus)\n",
        "    def __init__(self, corpus):\n",
        "        print(\"\"\"Your task is to implement four kinds of n-gram language models:\n",
        "      a) an (unsmoothed) unigram model (UnigramModel)\n",
        "      b) a unigram model smoothed using Laplace smoothing (SmoothedUnigramModel)\n",
        "      c) an unsmoothed bigram model (BigramModel)\n",
        "      d) a bigram model smoothed using linear interpolation smoothing (SmoothedBigramModelInt)\n",
        "      \"\"\")\n",
        "    #enddef\n",
        "\n",
        "    # Generate a sentence by drawing words according to the\n",
        "    # model's probability distribution\n",
        "    # Note: think about how to set the length of the sentence\n",
        "    #in a principled way\n",
        "    def generateSentence(self):\n",
        "        print(\"Implement the generateSentence method in each subclass\")\n",
        "        return \"mary had a little lamb .\"\n",
        "    #emddef\n",
        "\n",
        "    # Given a sentence (sen), return the probability of\n",
        "    # that sentence under the model\n",
        "    def getSentenceProbability(self, sen):\n",
        "        print(\"Implement the getSentenceProbability method in each subclass\")\n",
        "        return 0.0\n",
        "    #enddef\n",
        "\n",
        "    # Given a corpus, calculate and return its perplexity\n",
        "    #(normalized inverse log probability)\n",
        "    def getCorpusPerplexity(self, corpus):\n",
        "        print(\"Implement the getCorpusPerplexity method\")\n",
        "        return 0.0\n",
        "    #enddef\n",
        "\n",
        "    # Given a file (filename) and the number of sentences, generate a list\n",
        "    # of sentences and write each to file along with its model probability.\n",
        "    # Note: you shouldn't need to change this method\n",
        "    def generateSentencesToFile(self, numberOfSentences, filename):\n",
        "        filePointer = open(filename, 'w+')\n",
        "        for i in range(0,numberOfSentences):\n",
        "            sen = self.generateSentence()\n",
        "            prob = self.getSentenceProbability(sen)\n",
        "\n",
        "            stringGenerated = str(prob) + \" \" + \" \".join(sen)\n",
        "            print(stringGenerated, end=\"\\n\", file=filePointer)\n",
        "\n",
        "\t#endfor\n",
        "    #enddef\n",
        "#endclass\n",
        "\n",
        "# Unigram language model\n",
        "\n",
        "# prompt: class UnigramModel(LanguageModel):     def __init__(self, corpus):         print(\"Subtask: implement the unsmoothed unigram language model\")     #endddef #endclass\n",
        "\n",
        "class UnigramModel(LanguageModel):\n",
        "    def __init__(self, corpus):\n",
        "        print(\"Subtask: implement the unsmoothed unigram language model\")\n",
        "        self.unigramCounts = defaultdict(int)\n",
        "        self.total = 0\n",
        "        for sentence in corpus:\n",
        "            for word in sentence:\n",
        "                self.unigramCounts[word] += 1\n",
        "                self.total += 1\n",
        "        #end for\n",
        "    #enddef\n",
        "\n",
        "    def generateSentence(self):\n",
        "        sentence = []\n",
        "        while True:\n",
        "            word = random.choices(list(self.unigramCounts.keys()), weights=list(self.unigramCounts.values()))[0]\n",
        "            if word == end:\n",
        "                break\n",
        "            sentence.append(word)\n",
        "        #end while\n",
        "        return sentence\n",
        "    #enddef\n",
        "\n",
        "    def getSentenceProbability(self, sen):\n",
        "        prob = 1.0\n",
        "        for word in sen:\n",
        "            count = self.unigramCounts[word]\n",
        "            prob *= count / self.total\n",
        "        #end for\n",
        "        return prob\n",
        "    #enddef\n",
        "\n",
        "    def getCorpusPerplexity(self, corpus):\n",
        "        totalProb = 0.0\n",
        "        sentenceCount = 0\n",
        "        for sentence in corpus:\n",
        "            sentenceCount += 1\n",
        "            totalProb += math.log(self.getSentenceProbability(sentence))\n",
        "        #end for\n",
        "        return math.exp(-totalProb / sentenceCount)\n",
        "\n",
        "    def generateSentencesToFile(self, numberOfSentences, filename):\n",
        "        filePointer = open(filename, 'w+')\n",
        "        for i in range(0, numberOfSentences):\n",
        "            sen = self.generateSentence()\n",
        "            prob = self.getSentenceProbability(sen)\n",
        "\n",
        "            stringGenerated = str(prob) + \" \" + \" \".join(sen)\n",
        "            print(stringGenerated, end=\"\\n\", file=filePointer)\n",
        "\n",
        "        filePointer.close()\n",
        "    #enddef\n",
        "#endclass\n",
        "\n",
        "# prompt: complete the following code and function     def __init__(self, corpus):         print(\"Subtask: implement the smoothed bigram language model with kneser-ney smoothing\")     #endddef #endclass\n",
        "\n",
        "class SmoothedBigramModelKneserNey(LanguageModel):\n",
        "    def __init__(self, corpus):\n",
        "        print(\"Subtask: implement the smoothed bigram language model with kneser-ney smoothing\")\n",
        "        self.bigramCounts = defaultdict(lambda: defaultdict(int))\n",
        "        self.unigramCounts = defaultdict(int)\n",
        "        self.total = 0\n",
        "        for sentence in corpus:\n",
        "            for i in range(0, len(sentence) - 1):\n",
        "                word1 = sentence[i]\n",
        "                word2 = sentence[i + 1]\n",
        "                self.bigramCounts[word1][word2] += 1\n",
        "                self.unigramCounts[word1] += 1\n",
        "                self.total += 1\n",
        "            #end for\n",
        "        #end for\n",
        "    #enddef\n",
        "\n",
        "    def generateSentence(self):\n",
        "        sentence = []\n",
        "        while True:\n",
        "            word1 = random.choices(list(self.unigramCounts.keys()), weights=list(self.unigramCounts.values()))[0]\n",
        "            word2 = random.choices(list(self.bigramCounts[word1].keys()), weights=list(self.bigramCounts[word1].values()))[0]\n",
        "            if word2 == end:\n",
        "                break\n",
        "            sentence.append(word1)\n",
        "            sentence.append(word2)\n",
        "            word1 = word2\n",
        "        #end while\n",
        "        return sentence\n",
        "    #enddef\n",
        "\n",
        "    def getSentenceProbability(self, sen):\n",
        "        prob = 1.0\n",
        "        for i in range(0, len(sen) - 1):\n",
        "            word1 = sen[i]\n",
        "            word2 = sen[i + 1]\n",
        "            count = self.bigramCounts[word1][word2]\n",
        "            prob *= count / self.unigramCounts[word1]\n",
        "        #end for\n",
        "        return prob\n",
        "    #enddef\n",
        "\n",
        "    def getCorpusPerplexity(self, corpus):\n",
        "        totalProb = 0.0\n",
        "        sentenceCount = 0\n",
        "        for sentence in corpus:\n",
        "            sentenceCount += 1\n",
        "            totalProb += math.log(self.getSentenceProbability(sentence))\n",
        "        #end for\n",
        "        return math.exp(-totalProb / sentenceCount)\n",
        "\n",
        "    def generateSentencesToFile(self, numberOfSentences, filename):\n",
        "        filePointer = open(filename, 'w+')\n",
        "        for i in range(0, numberOfSentences):\n",
        "            sen = self.generateSentence()\n",
        "            prob = self.getSentenceProbability(sen)\n",
        "\n",
        "            stringGenerated = str(prob) + \" \" + \" \".join(sen)\n",
        "            print(stringGenerated, end=\"\\n\", file=filePointer)\n",
        "\n",
        "        filePointer.close()\n",
        "    #enddef\n",
        "#endclass\n",
        "\n",
        "\n",
        "smoothedBigramModelKneserNey = SmoothedBigramModelKneserNey(corpus)\n",
        "\n",
        "generatedSentence = smoothedBigramModelKneserNey.generateSentence()\n",
        "print(\"Generated sentence:\", \" \".join(generatedSentence))\n",
        "\n",
        "perplexity = smoothedBigramModelKneserNey.getCorpusPerplexity(corpus)\n",
        "print(\"Corpus perplexity:\", perplexity)\n",
        "\n",
        "smoothedBigramModelKneserNey.generateSentencesToFile(10, \"smoothed_bigram_sentences_kneser_ney.txt\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Smoothed unigram language model (use laplace for smoothing)\n",
        "# prompt: class SmoothedUnigramModel(LanguageModel):     def __init__(self, corpus):         print(\"Subtask: implement the smoothed unigram language model\")     #endddef #endclass\n",
        "\n",
        "class SmoothedUnigramModel(LanguageModel):\n",
        "    def __init__(self, corpus):\n",
        "        print(\"Subtask: implement the smoothed unigram language model\")\n",
        "        self.unigramCounts = defaultdict(int)\n",
        "        self.total = 0\n",
        "        for sentence in corpus:\n",
        "            for word in sentence:\n",
        "                self.unigramCounts[word] += 1\n",
        "                self.total += 1\n",
        "        #end for\n",
        "        self.V = len(self.unigramCounts)\n",
        "    #enddef\n",
        "\n",
        "    def generateSentence(self):\n",
        "        sentence = []\n",
        "        while True:\n",
        "            word = random.choices(list(self.unigramCounts.keys()), weights=list(self.unigramCounts.values()))[0]\n",
        "            if word == end:\n",
        "                break\n",
        "            sentence.append(word)\n",
        "        #end while\n",
        "        return sentence\n",
        "    #enddef\n",
        "\n",
        "    def getSentenceProbability(self, sen):\n",
        "        prob = 1.0\n",
        "        for word in sen:\n",
        "            count = self.unigramCounts[word]\n",
        "            prob *= (count + 1) / (self.total + self.V)\n",
        "        #end for\n",
        "        return prob\n",
        "    #enddef\n",
        "\n",
        "    def getCorpusPerplexity(self, corpus):\n",
        "        totalProb = 0.0\n",
        "        sentenceCount = 0\n",
        "        for sentence in corpus:\n",
        "            sentenceCount += 1\n",
        "            totalProb += math.log(self.getSentenceProbability(sentence))\n",
        "        #end for\n",
        "        return math.exp(-totalProb / sentenceCount)\n",
        "\n",
        "    def generateSentencesToFile(self, numberOfSentences, filename):\n",
        "        filePointer = open(filename, 'w+')\n",
        "        for i in range(0, numberOfSentences):\n",
        "            sen = self.generateSentence()\n",
        "            prob = self.getSentenceProbability(sen)\n",
        "\n",
        "            stringGenerated = str(prob) + \" \" + \" \".join(sen)\n",
        "            print(stringGenerated, end=\"\\n\", file=filePointer)\n",
        "\n",
        "        filePointer.close()\n",
        "    #enddef\n",
        "#endclass\n",
        "\n",
        "\n",
        "# Unsmoothed bigram language model\n",
        "# prompt: kindly do the SmoothedBigramModelKN with all functions\n",
        "\n",
        "class SmoothedBigramModelKN(LanguageModel):\n",
        "    def __init__(self, corpus):\n",
        "        print(\"Subtask: implement the smoothed bigram language model with kneser-ney smoothing\")\n",
        "        self.bigramCounts = defaultdict(lambda: defaultdict(int))\n",
        "        self.unigramCounts = defaultdict(int)\n",
        "        self.total = 0\n",
        "        for sentence in corpus:\n",
        "            for i in range(0, len(sentence) - 1):\n",
        "                word1 = sentence[i]\n",
        "                word2 = sentence[i + 1]\n",
        "                self.bigramCounts[word1][word2] += 1\n",
        "                self.unigramCounts[word1] += 1\n",
        "                self.total += 1\n",
        "            #end for\n",
        "        #end for\n",
        "        self.V = len(self.unigramCounts)\n",
        "    #enddef\n",
        "\n",
        "    def generateSentence(self):\n",
        "        sentence = []\n",
        "        while True:\n",
        "            word1 = random.choices(list(self.unigramCounts.keys()), weights=list(self.unigramCounts.values()))[0]\n",
        "            word2 = random.choices(list(self.bigramCounts[word1].keys()), weights=list(self.bigramCounts[word1].values()))[0]\n",
        "            if word2 == end:\n",
        "                break\n",
        "            sentence.append(word1)\n",
        "            sentence.append(word2)\n",
        "            word1 = word2\n",
        "        #end while\n",
        "        return sentence\n",
        "    #enddef\n",
        "\n",
        "    def getSentenceProbability(self, sen):\n",
        "        prob = 1.0\n",
        "        for i in range(0, len(sen) - 1):\n",
        "            word1 = sen[i]\n",
        "            word2 = sen[i + 1]\n",
        "            count = self.bigramCounts[word1][word2]\n",
        "            prob *= (count + 1) / (self.unigramCounts[word1] + self.V)\n",
        "        #end for\n",
        "        return prob\n",
        "    #enddef\n",
        "\n",
        "    def getCorpusPerplexity(self, corpus):\n",
        "        totalProb = 0.0\n",
        "        sentenceCount = 0\n",
        "        for sentence in corpus:\n",
        "            sentenceCount += 1\n",
        "            totalProb += math.log(self.getSentenceProbability(sentence))\n",
        "        #end for\n",
        "        return math.exp(-totalProb / sentenceCount)\n",
        "\n",
        "    def generateSentencesToFile(self, numberOfSentences, filename):\n",
        "        filePointer = open(filename, 'w+')\n",
        "        for i in range(0, numberOfSentences):\n",
        "            sen = self.generateSentence()\n",
        "            prob = self.getSentenceProbability(sen)\n",
        "\n",
        "            stringGenerated = str(prob) + \" \" + \" \".join(sen)\n",
        "            print(stringGenerated, end=\"\\n\", file=filePointer)\n",
        "\n",
        "        filePointer.close()\n",
        "    #enddef\n",
        "#endclass\n",
        "\n",
        "\n",
        "\n",
        "# Sample class for a unsmoothed unigram probability distribution\n",
        "# Note:\n",
        "#       Feel free to use/re-use/modify this class as necessary for your\n",
        "#       own code (e.g. converting to log probabilities after training).\n",
        "#       This class is intended to help you get started\n",
        "#       with your implementation of the language models above.\n",
        "\n",
        "\n",
        "class UnigramDist:\n",
        "    def __init__(self, corpus):\n",
        "        self.counts = defaultdict(float)\n",
        "        self.total = 0.0\n",
        "        self.train(corpus)\n",
        "    #endddef\n",
        "\n",
        "    # Add observed counts from corpus to the distribution\n",
        "    def train(self, corpus):\n",
        "        for sen in corpus:\n",
        "            for word in sen:\n",
        "                if word == start:\n",
        "                    continue\n",
        "                self.counts[word] += 1.0\n",
        "                self.total += 1.0\n",
        "            #endfor\n",
        "        #endfor\n",
        "    #enddef\n",
        "\n",
        "    # Returns the probability of word in the distribution\n",
        "    def prob(self, word):\n",
        "        return self.counts[word]/self.total\n",
        "    #enddef\n",
        "\n",
        "    # Generate a single random word according to the distribution\n",
        "    def draw(self):\n",
        "        rand = random.random()\n",
        "        for word in self.counts.keys():\n",
        "            rand -= self.prob(word)\n",
        "            if rand <= 0.0:\n",
        "                return word\n",
        "\t    #endif\n",
        "\t#endfor\n",
        "    #enddef\n",
        "#endclass\n",
        "\n",
        "\n",
        "unigram_dist = UnigramDist(corpus)\n",
        "\n",
        "print(unigram_dist.prob(\"the\"))\n",
        "\n",
        "print(unigram_dist.prob(\"aardvark\"))\n",
        "\n",
        "print(unigram_dist.draw())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#-------------------------------------------\n",
        "# The main routine\n",
        "#-------------------------------------------\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    #read your corpora\n",
        "    trainCorpus = readFileToCorpus('train.txt')\n",
        "    trainCorpus = preprocess(trainCorpus)\n",
        "\n",
        "    posTestCorpus = readFileToCorpus('pos_test.txt')\n",
        "    negTestCorpus = readFileToCorpus('neg_test.txt')\n",
        "\n",
        "\n",
        "\n",
        "    from collections import Counter\n",
        "    import re\n",
        "\n",
        "    def read_corpus(file_path):\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            lines = file.readlines()\n",
        "        # Preprocess the lines (remove leading/trailing whitespaces and split into words)\n",
        "        corpus = [re.findall(r'\\b\\w+\\b', line.lower()) for line in lines]\n",
        "        return corpus\n",
        "\n",
        "    def create_vocabulary(train_corpus):\n",
        "        # Flatten the list of sentences into a single list of words\n",
        "        all_words = [word for sentence in train_corpus for word in sentence]\n",
        "\n",
        "        # Use Counter to count the frequency of each word\n",
        "        word_counts = Counter(all_words)\n",
        "\n",
        "        # Create a vocabulary containing unique words\n",
        "        vocabulary = list(word_counts.keys())\n",
        "\n",
        "        return vocabulary\n",
        "\n",
        "    # Example usage:\n",
        "    train_file_path = 'train.txt'\n",
        "    train_corpus = read_corpus(train_file_path)\n",
        "    train_vocabulary = create_vocabulary(train_corpus)\n",
        "\n",
        "    print(\"Vocabulary Size:\", len(train_vocabulary))\n",
        "    print(\"First 10 Words in Vocabulary:\", train_vocabulary[:10])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #vocab = set()\n",
        "    # Please write the code to create the vocab over here before the function preprocessTest\n",
        "    #print(\"\"\"Task 0: create a vocabulary(collection of word types) for the train corpus\"\"\")\n",
        "\n",
        "\n",
        "    posTestCorpus = preprocessTest(train_vocabulary, posTestCorpus)\n",
        "    negTestCorpus = preprocessTest(train_vocabulary, negTestCorpus)\n",
        "\n",
        "    # Run sample unigram dist code\n",
        "    unigramDist = UnigramDist(trainCorpus)\n",
        "    print(\"Sample UnigramDist output:\")\n",
        "    print(\"Probability of \\\"picture\\\": \", unigramDist.prob(\"picture\"))\n",
        "    print(\"\\\"Random\\\" draw: \", unigramDist.draw())\n",
        "    print(\"\\\"Random\\\" draw: \", unigramDist.draw())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NQzJu2NlM9AT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}